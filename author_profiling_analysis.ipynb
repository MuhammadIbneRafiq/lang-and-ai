{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Author Profiling Data Analysis stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "print(\"Available datasets:\")\n",
    "for f in DATA_DIR.glob('*.csv'):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    if 'sample' not in f.name:\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Key Datasets for Author Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (sampling for efficiency)\n",
    "def load_sample(filename, n=10000):\n",
    "    df = pd.read_csv(DATA_DIR / filename, header=None, names=['text', 'label'])\n",
    "    total = len(df)\n",
    "    if len(df) > n:\n",
    "        df = df.sample(n=n, random_state=42)\n",
    "    return df, total\n",
    "\n",
    "# Key datasets for proposals\n",
    "gender_df, gender_total = load_sample('gender.csv')\n",
    "age_df, age_total = load_sample('birth_year.csv')\n",
    "pol_df, pol_total = load_sample('political_leaning.csv')\n",
    "\n",
    "print(f\"Gender: {gender_total:,} total samples, {len(gender_df):,} loaded\")\n",
    "print(f\"Birth Year: {age_total:,} total samples, {len(age_df):,} loaded\")\n",
    "print(f\"Political: {pol_total:,} total samples, {len(pol_df):,} loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview & Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Gender distribution\n",
    "ax = axes[0]\n",
    "gender_counts = gender_df['label'].value_counts()\n",
    "gender_counts.index = ['Male (0)' if x == 0 else 'Female (1)' for x in gender_counts.index]\n",
    "gender_counts.plot(kind='bar', ax=ax, color=['steelblue', 'coral'], edgecolor='black')\n",
    "ax.set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Birth year distribution\n",
    "ax = axes[1]\n",
    "age_df['label'].astype(int).hist(ax=ax, bins=40, color='seagreen', edgecolor='black')\n",
    "ax.set_title('Birth Year Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Birth Year')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Political leaning\n",
    "ax = axes[2]\n",
    "pol_counts = pol_df['label'].value_counts()\n",
    "colors = {'left': 'blue', 'center': 'gray', 'right': 'red'}\n",
    "pol_counts.plot(kind='bar', ax=ax, color=[colors.get(x, 'gray') for x in pol_counts.index], edgecolor='black')\n",
    "ax.set_title('Political Leaning Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('key_dataset_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label-Leaking Token Analysis (Proposal 3)\n",
    "\n",
    "Detecting explicit demographic mentions that models might use as shortcuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label-leaking patterns\n",
    "LEAK_PATTERNS = {\n",
    "    'age_gender_combo': r'\\b(\\d{1,2})\\s*[MFmf]\\b|\\b[MFmf]\\s*(\\d{1,2})\\b',  # \"18F\", \"M25\"\n",
    "    'i_am_age': r\"[Ii]'?m\\s+(\\d{1,2})\\b|[Ii]\\s+am\\s+(\\d{1,2})\\b\",  # \"I'm 18\"\n",
    "    'i_am_gender': r\"[Ii]'?m\\s+a?\\s*(male|female|man|woman|guy|girl)\\b\",\n",
    "    'age_years_old': r'\\b(\\d{1,2})\\s*(?:years?\\s*old|yo|y\\.o\\.)\\b',\n",
    "    'my_age_is': r'my\\s+age\\s+is\\s+(\\d{1,2})',\n",
    "    'as_a_gender': r'as\\s+a\\s+(male|female|man|woman|guy|girl)\\b',\n",
    "    'born_year': r'\\bborn\\s+(?:in\\s+)?(\\d{4})\\b',\n",
    "    'age_brackets': r'\\(\\s*(\\d{1,2})\\s*[MFmf]\\s*\\)|\\(\\s*[MFmf]\\s*(\\d{1,2})\\s*\\)',\n",
    "}\n",
    "\n",
    "def find_all_leaks(text):\n",
    "    \"\"\"Find all label-leaking patterns in text.\"\"\"\n",
    "    results = {}\n",
    "    for name, pattern in LEAK_PATTERNS.items():\n",
    "        matches = re.findall(pattern, str(text), re.IGNORECASE)\n",
    "        if matches:\n",
    "            results[name] = matches\n",
    "    return results\n",
    "\n",
    "def has_leak(text):\n",
    "    \"\"\"Check if text has any label-leaking token.\"\"\"\n",
    "    return len(find_all_leaks(text)) > 0\n",
    "\n",
    "# Analyze gender dataset\n",
    "print(\"Analyzing label leaking in datasets...\")\n",
    "gender_df['has_leak'] = gender_df['text'].apply(has_leak)\n",
    "age_df['has_leak'] = age_df['text'].apply(has_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label leaking prevalence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gender\n",
    "ax = axes[0]\n",
    "leak_by_gender = gender_df.groupby('label')['has_leak'].mean() * 100\n",
    "leak_by_gender.index = ['Male', 'Female']\n",
    "bars = ax.bar(leak_by_gender.index, leak_by_gender.values, color=['steelblue', 'coral'], edgecolor='black')\n",
    "ax.set_ylabel('% with Label-Leaking Tokens')\n",
    "ax.set_title('Label Leaking by Gender', fontsize=12, fontweight='bold')\n",
    "for bar, val in zip(bars, leak_by_gender.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', \n",
    "            ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylim(0, max(leak_by_gender) * 1.2)\n",
    "\n",
    "# Overall\n",
    "ax = axes[1]\n",
    "datasets = ['Gender', 'Birth Year']\n",
    "leak_rates = [\n",
    "    gender_df['has_leak'].mean() * 100,\n",
    "    age_df['has_leak'].mean() * 100\n",
    "]\n",
    "bars = ax.bar(datasets, leak_rates, color=['mediumpurple', 'seagreen'], edgecolor='black')\n",
    "ax.set_ylabel('% with Label-Leaking Tokens')\n",
    "ax.set_title('Label Leaking Prevalence by Dataset', fontsize=12, fontweight='bold')\n",
    "for bar, val in zip(bars, leak_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', \n",
    "            ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylim(0, max(leak_rates) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('label_leaking_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGender dataset: {gender_df['has_leak'].mean()*100:.1f}% samples have label-leaking tokens\")\n",
    "print(f\"Birth Year dataset: {age_df['has_leak'].mean()*100:.1f}% samples have label-leaking tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of label-leaking tokens\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLES OF LABEL-LEAKING TOKENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "leak_examples = gender_df[gender_df['has_leak']].head(5)\n",
    "for idx, row in leak_examples.iterrows():\n",
    "    leaks = find_all_leaks(row['text'])\n",
    "    label = 'Female' if row['label'] == 1 else 'Male'\n",
    "    print(f\"\\n[{label}] Found leaks: {leaks}\")\n",
    "    # Show snippet around leak\n",
    "    text = str(row['text'])[:500]\n",
    "    print(f\"Snippet: {text}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stylometric Feature Extraction (Proposal 2)\n",
    "\n",
    "Extract stylometric features that might be stable across domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"Extract stylometric features from text.\"\"\"\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Length features\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(words)\n",
    "    features['sentence_count'] = max(len(sentences), 1)\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    features['avg_sentence_length'] = features['word_count'] / features['sentence_count']\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    unique_words = set(w.lower() for w in words)\n",
    "    features['vocab_richness'] = len(unique_words) / max(len(words), 1)\n",
    "    \n",
    "    # Punctuation features\n",
    "    punct_counts = Counter(c for c in text if c in string.punctuation)\n",
    "    total_punct = sum(punct_counts.values())\n",
    "    features['punct_ratio'] = total_punct / max(len(text), 1)\n",
    "    features['exclamation_ratio'] = punct_counts.get('!', 0) / max(total_punct, 1)\n",
    "    features['question_ratio'] = punct_counts.get('?', 0) / max(total_punct, 1)\n",
    "    features['comma_ratio'] = punct_counts.get(',', 0) / max(total_punct, 1)\n",
    "    \n",
    "    # Capitalization\n",
    "    features['caps_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    \n",
    "    # Function word approximations\n",
    "    function_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                     'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',\n",
    "                     'i', 'you', 'he', 'she', 'it', 'we', 'they', 'my', 'your'}\n",
    "    lower_words = [w.lower() for w in words]\n",
    "    features['function_word_ratio'] = sum(1 for w in lower_words if w in function_words) / max(len(words), 1)\n",
    "    \n",
    "    # Pronoun usage\n",
    "    first_person = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours'}\n",
    "    second_person = {'you', 'your', 'yours', 'yourself'}\n",
    "    features['first_person_ratio'] = sum(1 for w in lower_words if w in first_person) / max(len(words), 1)\n",
    "    features['second_person_ratio'] = sum(1 for w in lower_words if w in second_person) / max(len(words), 1)\n",
    "    \n",
    "    # Sentence length variance\n",
    "    sent_lengths = [len(s.split()) for s in sentences]\n",
    "    features['sentence_length_var'] = np.var(sent_lengths) if len(sent_lengths) > 1 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for samples\n",
    "print(\"Extracting stylometric features...\")\n",
    "gender_features = pd.DataFrame([extract_stylometric_features(t) for t in gender_df['text']])\n",
    "gender_features['label'] = gender_df['label'].values\n",
    "\n",
    "age_features = pd.DataFrame([extract_stylometric_features(t) for t in age_df['text']])\n",
    "age_features['label'] = age_df['label'].values\n",
    "\n",
    "print(f\"Extracted {len(gender_features.columns)-1} stylometric features\")\n",
    "gender_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions by gender\n",
    "feature_cols = [c for c in gender_features.columns if c != 'label']\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    male_vals = gender_features[gender_features['label'] == 0][feat]\n",
    "    female_vals = gender_features[gender_features['label'] == 1][feat]\n",
    "    \n",
    "    ax.hist(male_vals, bins=30, alpha=0.6, label='Male', color='steelblue', density=True)\n",
    "    ax.hist(female_vals, bins=30, alpha=0.6, label='Female', color='coral', density=True)\n",
    "    ax.set_title(feat.replace('_', ' ').title(), fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Hide extra axes\n",
    "for idx in range(len(feature_cols), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stylometric_features_by_gender.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of features by gender\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STYLOMETRIC FEATURE COMPARISON BY GENDER\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<25} {'Male Mean':>12} {'Female Mean':>12} {'Diff %':>10} {'p-value':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = []\n",
    "for feat in feature_cols:\n",
    "    male = gender_features[gender_features['label'] == 0][feat]\n",
    "    female = gender_features[gender_features['label'] == 1][feat]\n",
    "    \n",
    "    male_mean = male.mean()\n",
    "    female_mean = female.mean()\n",
    "    diff_pct = ((female_mean - male_mean) / male_mean * 100) if male_mean != 0 else 0\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_val = stats.ttest_ind(male, female)\n",
    "    \n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "    print(f\"{feat:<25} {male_mean:>12.4f} {female_mean:>12.4f} {diff_pct:>+9.1f}% {p_val:>10.2e} {sig}\")\n",
    "    \n",
    "    results.append({\n",
    "        'feature': feat,\n",
    "        'male_mean': male_mean,\n",
    "        'female_mean': female_mean,\n",
    "        'diff_pct': diff_pct,\n",
    "        'p_value': p_val\n",
    "    })\n",
    "\n",
    "print(\"\\n* p<0.05, ** p<0.01, *** p<0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = gender_features[feature_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Stylometric Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Baseline: Feature Predictive Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = gender_features[feature_cols].values\n",
    "y = gender_features['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE: Stylometric Features for Gender Prediction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, lr_pred):.3f}\")\n",
    "print(f\"  Macro F1: {f1_score(y_test, lr_pred, average='macro'):.3f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, rf_pred):.3f}\")\n",
    "print(f\"  Macro F1: {f1_score(y_test, rf_pred, average='macro'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "ax = axes[0]\n",
    "coef_df = pd.DataFrame({'feature': feature_cols, 'coef': np.abs(lr.coef_[0])})\n",
    "coef_df = coef_df.sort_values('coef', ascending=True)\n",
    "ax.barh(coef_df['feature'], coef_df['coef'], color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('|Coefficient|')\n",
    "ax.set_title('Logistic Regression Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Random Forest importance\n",
    "ax = axes[1]\n",
    "imp_df = pd.DataFrame({'feature': feature_cols, 'importance': rf.feature_importances_})\n",
    "imp_df = imp_df.sort_values('importance', ascending=True)\n",
    "ax.barh(imp_df['feature'], imp_df['importance'], color='seagreen', edgecolor='black')\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Random Forest Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Impact of Removing Label-Leaking Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label_leaking_tokens(text):\n",
    "    \"\"\"Remove label-leaking tokens from text.\"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove age-gender combos like \"18F\", \"M25\", \"(21F)\"\n",
    "    text = re.sub(r'\\(\\s*\\d{1,2}\\s*[MFmf]\\s*\\)', '', text)\n",
    "    text = re.sub(r'\\(\\s*[MFmf]\\s*\\d{1,2}\\s*\\)', '', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s*[MFmf]\\b', '', text)\n",
    "    text = re.sub(r'\\b[MFmf]\\s*\\d{1,2}\\b', '', text)\n",
    "    \n",
    "    # Remove \"I'm X years old\" patterns\n",
    "    text = re.sub(r\"[Ii]'?m\\s+\\d{1,2}\\b\", \"I'm [AGE]\", text)\n",
    "    text = re.sub(r\"[Ii]\\s+am\\s+\\d{1,2}\\b\", \"I am [AGE]\", text)\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s*(?:years?\\s*old|yo|y\\.o\\.)', '[AGE]', text)\n",
    "    \n",
    "    # Remove gender mentions\n",
    "    text = re.sub(r\"[Ii]'?m\\s+a?\\s*(male|female)\\b\", \"I'm [GENDER]\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"as\\s+a\\s+(male|female|man|woman|guy|girl)\\b\", \"as a [GENDER]\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean up extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Create cleaned version\n",
    "gender_df['text_clean'] = gender_df['text'].apply(remove_label_leaking_tokens)\n",
    "\n",
    "# Example\n",
    "print(\"Example of label-leaking token removal:\")\n",
    "print(\"=\"*60)\n",
    "example_idx = gender_df[gender_df['has_leak']].index[0]\n",
    "print(f\"BEFORE: {gender_df.loc[example_idx, 'text'][:300]}...\")\n",
    "print(f\"\\nAFTER:  {gender_df.loc[example_idx, 'text_clean'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare features before and after cleaning\n",
    "print(\"Extracting features from cleaned text...\")\n",
    "gender_features_clean = pd.DataFrame([extract_stylometric_features(t) for t in gender_df['text_clean']])\n",
    "gender_features_clean['label'] = gender_df['label'].values\n",
    "\n",
    "# Train on clean data\n",
    "X_clean = gender_features_clean[feature_cols].values\n",
    "y_clean = gender_features_clean['label'].values\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean)\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
    "\n",
    "# Train models\n",
    "lr_clean = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clean.fit(X_train_c_scaled, y_train_c)\n",
    "lr_pred_clean = lr_clean.predict(X_test_c_scaled)\n",
    "\n",
    "rf_clean = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clean.fit(X_train_c, y_train_c)\n",
    "rf_pred_clean = rf_clean.predict(X_test_c)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Original vs Cleaned Text (Stylometric Features)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<25} {'Original Acc':>15} {'Cleaned Acc':>15} {'Change':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "orig_lr_acc = accuracy_score(y_test, lr_pred)\n",
    "clean_lr_acc = accuracy_score(y_test_c, lr_pred_clean)\n",
    "print(f\"{'Logistic Regression':<25} {orig_lr_acc:>15.3f} {clean_lr_acc:>15.3f} {(clean_lr_acc-orig_lr_acc):>+10.3f}\")\n",
    "\n",
    "orig_rf_acc = accuracy_score(y_test, rf_pred)\n",
    "clean_rf_acc = accuracy_score(y_test_c, rf_pred_clean)\n",
    "print(f\"{'Random Forest':<25} {orig_rf_acc:>15.3f} {clean_rf_acc:>15.3f} {(clean_rf_acc-orig_rf_acc):>+10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is what chat says"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVAILABLE DATASETS:\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "  • gender.csv (44K samples) - Binary: Male(0)/Female(1) - BALANCED\n",
    "  • birth_year.csv (42K samples) - 58 years (1960s-2000s) - Peak: 1990s\n",
    "  • political_leaning.csv (57K samples) - 3 classes: center/right/left\n",
    "  • nationality.csv (83K samples) - 51 countries - Top: Germany/UK/USA\n",
    "  • MBTI datasets (40K each) - 4 binary dimensions - IMBALANCED\n",
    "\n",
    "KEY FINDINGS FOR PROPOSAL 2 (Stylometric Features):\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "  • Extracted 14 stylometric features (length, punctuation, pronouns, etc.)\n",
    "  • Several features show significant gender differences (p<0.001)\n",
    "  • Baseline accuracy with stylometric features: ~53-55%\n",
    "  • Function word ratio & pronoun usage are most predictive\n",
    "\n",
    "KEY FINDINGS FOR PROPOSAL 3 (Label-Leaking Tokens):\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "  • ~44% of gender samples contain explicit label mentions (\"18F\", \"I'm male\")\n",
    "  • ~42% of birth_year samples contain label-leaking tokens\n",
    "  • These provide strong shortcuts for models to exploit\n",
    "  • Perfect dataset for studying shortcut learning\n",
    "\n",
    "RECOMMENDED DATASETS FOR YOUR PROPOSALS:\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "  PRIMARY: gender.csv + birth_year.csv\n",
    "    → High label-leaking prevalence\n",
    "    → Clear binary/ordinal labels\n",
    "    → Sufficient sample size\n",
    "  \n",
    "  SECONDARY: political_leaning.csv\n",
    "    → Less label leaking (topic-based)\n",
    "    → Good for cross-domain robustness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned samples for experiments\n",
    "print(\"Saving cleaned samples for experiments...\")\n",
    "\n",
    "# Gender with both versions\n",
    "gender_export = gender_df[['text', 'text_clean', 'label', 'has_leak']].copy()\n",
    "gender_export.to_csv('data/gender_analysis_ready.csv', index=False)\n",
    "print(f\"Saved: data/gender_analysis_ready.csv ({len(gender_export)} rows)\")\n",
    "\n",
    "# Feature datasets\n",
    "gender_features.to_csv('data/gender_stylometric_features.csv', index=False)\n",
    "print(f\"Saved: data/gender_stylometric_features.csv ({len(gender_features)} rows)\")\n",
    "\n",
    "print(\"\\nDone! Ready for experiments.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

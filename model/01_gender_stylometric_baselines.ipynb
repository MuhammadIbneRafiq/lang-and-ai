{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender stylometric baselines\n",
    "\n",
    "Train simple baselines for gender prediction using stylometric features only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "DATA_DIR_CANDIDATES = [\n",
    "    Path(\"data\"),\n",
    "    Path(\"..\") / \"data\",\n",
    "    Path(\"C:/Users/muham/OneDrive - TU Eindhoven/q2/lang-and-ai/data\"),\n",
    "]\n",
    "\n",
    "for p in DATA_DIR_CANDIDATES:\n",
    "    if p.exists():\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find data directory. Checked: \" + \", \\".join(str(p) for p in DATA_DIR_CANDIDATES)\n",
    "    )\n",
    "\n",
    "print(\"Using DATA_DIR:\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and subsample gender dataset\n",
    "gender_df = pd.read_csv(DATA_DIR / \"gender.csv\", header=None, names=[\"text\", \"label\"])\n",
    "gender_df = gender_df[gender_df[\"label\"].isin([0, 1, \"0\", \"1\"])].copy()\n",
    "gender_df[\"label\"] = gender_df[\"label\"].astype(int)\n",
    "gender_df = gender_df.sample(n=min(10000, len(gender_df)), random_state=42).reset_index(drop=True)\n",
    "print(gender_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    features = {}\n",
    "    features[\"char_count\"] = len(text)\n",
    "    features[\"word_count\"] = len(words)\n",
    "    features[\"sentence_count\"] = max(len(sentences), 1)\n",
    "    features[\"avg_word_length\"] = np.mean([len(w) for w in words]) if words else 0\n",
    "    features[\"avg_sentence_length\"] = features[\"word_count\"] / features[\"sentence_count\"]\n",
    "\n",
    "    unique_words = set(w.lower() for w in words)\n",
    "    features[\"vocab_richness\"] = len(unique_words) / max(len(words), 1)\n",
    "\n",
    "    punct_counts = Counter(c for c in text if c in string.punctuation)\n",
    "    total_punct = sum(punct_counts.values())\n",
    "    features[\"punct_ratio\"] = total_punct / max(len(text), 1)\n",
    "    features[\"exclamation_ratio\"] = punct_counts.get(\"!\", 0) / max(total_punct, 1)\n",
    "    features[\"question_ratio\"] = punct_counts.get(\"?\", 0) / max(total_punct, 1)\n",
    "    features[\"comma_ratio\"] = punct_counts.get(\",\", 0) / max(total_punct, 1)\n",
    "    features[\"caps_ratio\"] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "\n",
    "    function_words = {\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"to\",\n",
    "        \"of\",\n",
    "        \"in\",\n",
    "        \"for\",\n",
    "        \"on\",\n",
    "        \"with\",\n",
    "        \"at\",\n",
    "        \"by\",\n",
    "        \"from\",\n",
    "        \"i\",\n",
    "        \"you\",\n",
    "        \"he\",\n",
    "        \"she\",\n",
    "        \"it\",\n",
    "        \"we\",\n",
    "        \"they\",\n",
    "        \"my\",\n",
    "        \"your\",\n",
    "    }\n",
    "    lower_words = [w.lower() for w in words]\n",
    "    features[\"function_word_ratio\"] = sum(1 for w in lower_words if w in function_words) / max(len(words), 1)\n",
    "\n",
    "    first_person = {\"i\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\"}\n",
    "    second_person = {\"you\", \"your\", \"yours\", \"yourself\"}\n",
    "    features[\"first_person_ratio\"] = sum(1 for w in lower_words if w in first_person) / max(len(words), 1)\n",
    "    features[\"second_person_ratio\"] = sum(1 for w in lower_words if w in second_person) / max(len(words), 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and train baselines\n",
    "gender_features = pd.DataFrame([extract_stylometric_features(t) for t in gender_df[\"text\"]])\n",
    "gender_features[\"label\"] = gender_df[\"label\"].values\n",
    "feature_cols = [c for c in gender_features.columns if c != \"label\"]\n",
    "\n",
    "X = gender_features[feature_cols].values\n",
    "y = gender_features[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"  Accuracy:\", round(accuracy_score(y_test, lr_pred), 3))\n",
    "print(\"  Macro F1:\", round(f1_score(y_test, lr_pred, average=\"macro\"), 3))\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"  Accuracy:\", round(accuracy_score(y_test, rf_pred), 3))\n",
    "print(\"  Macro F1:\", round(f1_score(y_test, rf_pred, average=\"macro\"), 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

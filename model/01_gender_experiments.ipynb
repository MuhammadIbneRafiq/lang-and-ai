{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender experiments: stylometric + TF-IDF baselines\n",
    "\n",
    "Uses the Reddit author profiling `gender.csv` dataset to run:\n",
    "- Stylometric-feature baselines (LogReg + RandomForest)\\n\n",
    "- TF-IDF + LinearSVC on original vs leak-cleaned text\\n\n",
    "Data is read from the `data` directory in this repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Resolve data directory (works both from repo root and from model/)\n",
    "DATA_DIR_CANDIDATES = [\n",
    "    Path('data'),\n",
    "    Path('..') / 'data',\n",
    "    Path('C:/Users/muham/OneDrive - TU Eindhoven/q2/lang-and-ai/data'),\n",
    "]\n",
    "\n",
    "for p in DATA_DIR_CANDIDATES:\n",
    "    if p.exists():\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find data directory. Checked: \" + ', '.join(str(p) for p in DATA_DIR_CANDIDATES))\n",
    "\n",
    "print('Using DATA_DIR:', DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and subsample gender dataset (no header in CSV)\n",
    "gender_df = pd.read_csv(DATA_DIR / 'gender.csv', header=None, names=['text', 'label'])\n",
    "\n",
    "# Keep only binary labels and cast to int\n",
    "gender_df = gender_df[gender_df['label'].isin([0, 1, '0', '1'])].copy()\n",
    "gender_df['label'] = gender_df['label'].astype(int)\n",
    "\n",
    "# Subsample for faster experiments\n",
    "gender_df = gender_df.sample(n=min(20000, len(gender_df)), random_state=42).reset_index(drop=True)\n",
    "print('Dataset shape:', gender_df.shape)\n",
    "print(gender_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    features = {}\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(words)\n",
    "    features['sentence_count'] = max(len(sentences), 1)\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0.0\n",
    "    features['avg_sentence_length'] = features['word_count'] / features['sentence_count']\n",
    "\n",
    "    unique_words = set(w.lower() for w in words)\n",
    "    features['vocab_richness'] = len(unique_words) / max(len(words), 1)\n",
    "\n",
    "    punct_counts = Counter(c for c in text if c in string.punctuation)\n",
    "    total_punct = sum(punct_counts.values())\n",
    "    features['punct_ratio'] = total_punct / max(len(text), 1)\n",
    "    features['exclamation_ratio'] = punct_counts.get('!', 0) / max(total_punct, 1)\n",
    "    features['question_ratio'] = punct_counts.get('?', 0) / max(total_punct, 1)\n",
    "    features['comma_ratio'] = punct_counts.get(',', 0) / max(total_punct, 1)\n",
    "    features['caps_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "\n",
    "    function_words = {\n",
    "        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'to', 'of', 'in',\n",
    "        'for', 'on', 'with', 'at', 'by', 'from', 'i', 'you', 'he', 'she', 'it', 'we',\n",
    "        'they', 'my', 'your'\n",
    "    }\n",
    "    lower_words = [w.lower() for w in words]\n",
    "    features['function_word_ratio'] = sum(1 for w in lower_words if w in function_words) / max(len(words), 1)\n",
    "\n",
    "    first_person = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours'}\n",
    "    second_person = {'you', 'your', 'yours', 'yourself'}\n",
    "    features['first_person_ratio'] = sum(1 for w in lower_words if w in first_person) / max(len(words), 1)\n",
    "    features['second_person_ratio'] = sum(1 for w in lower_words if w in second_person) / max(len(words), 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylometric features and baselines\n",
    "gender_features = pd.DataFrame([extract_stylometric_features(t) for t in gender_df['text']])\n",
    "gender_features['label'] = gender_df['label'].values\n",
    "feature_cols = [c for c in gender_features.columns if c != 'label']\n",
    "\n",
    "X = gender_features[feature_cols].values\n",
    "y = gender_features['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print('Stylometric baselines (gender)')\n",
    "print('Logistic Regression - Acc:', round(accuracy_score(y_test, y_pred_lr), 3),\n",
    "      'Macro F1:', round(f1_score(y_test, y_pred_lr, average='macro'), 3))\n",
    "print('Random Forest      - Acc:', round(accuracy_score(y_test, y_pred_rf), 3),\n",
    "      'Macro F1:', round(f1_score(y_test, y_pred_rf, average='macro'), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-leaking patterns and cleaned text\n",
    "LEAK_PATTERNS = {\n",
    "    'age_gender_combo': r\\"\\b(\\d{1,2})\\s*[MFmf]\\b|\\b[MFmf]\\s*(\\d{1,2})\\b\\",\n",
    "    'i_am_age': r\\"[Ii]'?m\\s+(\\d{1,2})\\b|[Ii]\\s+am\\s+(\\d{1,2})\\b\\",\n",
    "    'i_am_gender': r\\"[Ii]'?m\\s+a?\\s*(male|female|man|woman|guy|girl)\\b\\",\n",
    "    'age_years_old': r\\"\\b(\\d{1,2})\\s*(?:years?\\s*old|yo|y\\.o\\.)\\b\\",\n",
    "    'age_brackets': r\\"\\(\\s*(\\d{1,2})\\s*[MFmf]\\s*\\)|\\(\\s*[MFmf]\\s*(\\d{1,2})\\s*\\)\\",\n",
    "}\n",
    "\n",
    "pattern_union = re.compile('|' . join(f'({p})' for p in LEAK_PATTERNS.values()), re.IGNORECASE)\n",
    "\n",
    "def remove_leak_tokens(text):\n",
    "    return pattern_union.sub(' ', str(text))\n",
    "\n",
    "gender_df['text_clean'] = gender_df['text'].apply(remove_leak_tokens)\n",
    "gender_df[['text', 'text_clean']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + LinearSVC on original vs cleaned text\n",
    "df_shuffled = gender_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df_shuffled))\n",
    "train_df = df_shuffled.iloc[:split_idx]\n",
    "test_df = df_shuffled.iloc[split_idx:]\n",
    "\n",
    "# Original text model\n",
    "vec_raw = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=5)\n",
    "X_train_raw = vec_raw.fit_transform(train_df['text'])\n",
    "X_test_raw = vec_raw.transform(test_df['text'])\n",
    "\n",
    "clf_raw = LinearSVC()\n",
    "clf_raw.fit(X_train_raw, train_df['label'])\n",
    "y_pred_raw = clf_raw.predict(X_test_raw)\n",
    "\n",
    "# Cleaned text model\n",
    "vec_clean = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=5)\n",
    "X_train_clean = vec_clean.fit_transform(train_df['text_clean'])\n",
    "X_test_clean = vec_clean.transform(test_df['text_clean'])\n",
    "\n",
    "clf_clean = LinearSVC()\n",
    "clf_clean.fit(X_train_clean, train_df['label'])\n",
    "y_pred_clean = clf_clean.predict(X_test_clean)\n",
    "\n",
    "def print_scores(name, y_true, y_pred):\n",
    "    print(name)\n",
    "    print('  Accuracy:', round(accuracy_score(y_true, y_pred), 3))\n",
    "    print('  Macro F1:', round(f1_score(y_true, y_pred, average='macro'), 3))\n",
    "\n",
    "print('\nTF-IDF + LinearSVC (gender)')\n",
    "print_scores('Original text (with potential label leaks):', test_df['label'], y_pred_raw)\n",
    "print()\n",
    "print_scores('Cleaned text (leak patterns removed):', test_df['label'], y_pred_clean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
